{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "幾乎所有的深度學習架構背後的設計核心都是張量和計算圖, PyTorch 也不例外, 本章我們將學習 PyTorch 中的張量系統 (Tensor) 和自動微分系統 (autograd).\n",
    "\n",
    "## <font color='darkblue'>Tensor</font>\n",
    "Tensor 又名張量, 它不僅在 PyTorch 中出現過, 也是 Theano, TensorFlow, Torch 和 MXNet 中重要的資料結構. 關於張量的本質不乏深度剖析的文章, 但從工程角度講, 可簡單地認為它就是一個陣列, 且支援高效的科學運算. 它可以是一個數 (純量), 一維陣列 (向量), 二維陣列 (矩陣) 或更高維的陣列 (高階資料). Tensor 和 <b><a href='http://www.numpy.org/'>numpy</a></b> 的 ndarrays 類似, 但 PyTorch 的 tensor 支援 GPU 加速.\n",
    "\n",
    "本節將系統說明 tensor 的使用, 力求面面俱到, 但不會涉及每個函數. 對於更多函數及其用法, 讀者可透過 IPython/Notebook 中使用 <function>? 檢視說明文件或參考 PyTorch 官方 API 文件.\n",
    "    \n",
    "### <font color='darkgreen'>基礎操作</font>\n",
    "學習過 numpy 的讀者會對本節內容非常熟悉, 因為 tensor 的介面設計與 <b><a href='http://www.numpy.org/'>numpy</a></b> 類似, 以方便使用者使用. 若不熟悉 <b><a href='http://www.numpy.org/'>numpy</a></b> 也沒關係, 本節內容會介紹. 從介面角度講, 對 tensor 的操作可以分為兩種:\n",
    "1. <a href='https://pytorch.org/docs/stable/torch.html'><b>torch</b></a> 上 function, 例如 <a href='https://pytorch.org/docs/stable/torch.html#torch.save'><b>torch</b>.save</a> 等.\n",
    "2. <a href='https://pytorch.org/docs/stable/tensors.html'><b>tensor</b></a> 上的 function, 如 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view'><b>tensor</b>.view</a> 等.\n",
    "\n",
    "為方便使用, 對 tensor 的大部分操作同時支援這兩個介面, 在這裡如 <font color='blue'>torch.sum(a, b)</font> 與 <font color='blue'>a.sum(b)</font> 功能相同. 從儲存的角度講, 對 tensor 的操作又可分成兩種:\n",
    "1. 不會修改本身的資料, 如 <font color='blue'>a.add(b)</font>, 加法的結果會回傳一個新的 tensor.\n",
    "2. 會修改本身資料 (inplace), 如 <font color='blue'>a.add_(b)</font>, 加法的結果會儲存到變數 a 中.\n",
    "\n",
    "函數名稱以 _ 結尾的都是 inplace 操作, 即會修改呼叫者自己本身的資料.\n",
    "\n",
    "### 建立 Tensor\n",
    "在 PyTorch 中新增 tensor 的方法有很多, 常見的方法整理如下:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| Tensor(*size) | 建構函式 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.ones'>ones(*size)</a> | 全部為 1 且長度為 <i>size</i> 的 Tensor |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.zeros'>zeros(*size)</a> | 全部為 0 且長度為 <i>size</i> 的 Tensor |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.eye'>eye(*size)</a> | 對角線為 1, 其他為 0 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.arange'>arange(s, e, step)</a> | 從 <i>s</i> 到 <i>e</i> 取 步進值 為 <i>step</i> |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.linspace'>linspace(s, e, steps)</a> | 從 <i>s</i> 到 <i>e</i> 均勻分成 <i>steps</i> 份 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.rand'>rand</a>/<a href='https://pytorch.org/docs/stable/torch.html#torch.randn'>randn(*size)</a> | 均勻/標準分布 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.normal'>normal(mean std)</a> / uniform(from, to) | 常態分布 / 均勻分布 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.randperm'>randperm(m)</a> | 隨機排列 |\n",
    "\n",
    "<br/>\n",
    "首先來介紹透過 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor'><b>Tensor</b></a> 建構子建立的 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor'><b>Tensor</b></a> 實例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "\n",
    "# 指定 tensor 的形狀為 2 rows, 3 columns. 內容值為記憶體當時的值\n",
    "a = t.Tensor(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 list 建立 tensor\n",
    "b = t.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把 tensor 轉回 list\n",
    "b.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size'>tensor.size()</a> 傳回 <font color='blue'><b>torch.Size</b></font> 物件, 它是 tuple 的字類別:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 傳回 b 中的元素數目: 2 * 3 = 6\n",
    "b.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.8855e+08,  8.3658e-43,  0.0000e+00],\n",
       "         [ 0.0000e+00,  1.4013e-45,  6.0000e+00]]), tensor([2., 3.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 創建一個與 b 形狀一樣的 tensor\n",
    "c = t.Tensor(b_size)\n",
    "\n",
    "# 創建一個元素為 2 和 3 的 tensor\n",
    "d = t.Tensor((2, 3))\n",
    "\n",
    "c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size'>tensor.size()</a>, 還可以利用 tensor.shape 直接檢視 tensor 的形狀:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是 t.Tensor(*size) 建立 tensor 時, 系統不會馬上分配空間, 只會計算剩餘的記憶體是否足夠使用, 使用到 tensor 時才會分配, 而其他操作都是在建立 tensor 後馬上進行空間分配. 其他常用的建立 tensor 方式舉例如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0099, -0.2518, -0.2731],\n",
       "        [-0.3166,  0.2981, -0.3879]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 2, 1, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5) # 長度為 5 的隨機排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用的 Tensor 操作\n",
    "透過 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view'>tensor.view</a> 方法可以修改 tensor 的形狀, 但必須確定調整前後元素總數一致. view 不會修改本身的資料, 傳回的新 tensor 與原來的 tensor 共用記憶體, 即更改其中一個, 另一個也會跟著改變. 在實際應用場合可能經常需要增加或減少某一維度, 這時 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.squeeze'>squeeze</a> 和 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unsqueeze'>unsqueeze</a> 兩個函數就派上用場."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 當某一維度為 -1 時, 會自動計算它的大小\n",
    "b = a.view(-1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在第一維 (下標從 0 開始) 上增加 \"1\"\n",
    "b1 = b.unsqueeze(1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -2 表示倒數第二個維度\n",
    "b2 = b.unsqueeze(-2)\n",
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0, 1, 2],\n",
       "           [3, 4, 5]]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 2],\n",
       "          [3, 4, 5]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 壓縮第 0 維\n",
    "c1 = c.squeeze(0)\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把所有維度為 \"1\" 進行壓縮 \n",
    "c2 = c.squeeze()\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0, 100,   2,   3,   4,   5]), tensor([[  0, 100,   2],\n",
       "         [  3,   4,   5]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "\n",
    "# b 與 a 共用記憶體, 修改了 a, b 也會改變\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize 是另一種可用來調整 size 的方法, 但與 view 不同的是它可以 tensor 的尺寸. 如果新尺寸超過原尺寸, 會自動分配新的記憶體空間; 而如果尺寸小於原尺寸, 則之前的資料依舊會被儲存, 先來看一個範例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[            0,           100,             2],\n",
       "        [            3,             4,             5],\n",
       "        [            0,             0, 2567799701136]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 舊資料依舊保存著, 多出來的空間則為分配到的記憶體空間的值\n",
    "b.resize_(3, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引操作\n",
    "<a href='https://pytorch.org/docs/stable/tensors.html#'><b>Tensor</b></a> 支援與 <a href='https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html'><b>numpy.ndarray</b></a> 類似的索引操作, 語法上也類似, 下面透過範例說明常用的索引操作. 如無特殊說明, 索引出來的結果與 Tensor 共用記憶體, 即修改一個會反映到另一個:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0337,  1.2891,  0.5059,  0.0979],\n",
       "        [ 1.1066, -1.1860,  0.2501, -1.4673],\n",
       "        [ 0.5826, -1.1911, -2.6953, -2.2500]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0337,  1.2891,  0.5059,  0.0979])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行 (下標從 0 開始)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0337,  1.1066,  0.5826])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 列\n",
    "a[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5059)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行; 第 2 列 上的元素. 等同 a[0][2]\n",
    "a[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0979)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行, 最後一個元素\n",
    "a[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0337,  1.2891,  0.5059,  0.0979],\n",
       "        [ 1.1066, -1.1860,  0.2501, -1.4673]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前兩行\n",
    "a[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0337,  1.2891],\n",
       "        [ 1.1066, -1.1860]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前兩行, 第 0, 1 列\n",
    "a[:2, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0337,  1.2891]])\n",
      "tensor([-0.0337,  1.2891])\n"
     ]
    }
   ],
   "source": [
    "print(a[0:1, :2])  # 第 0 行, 前兩列\n",
    "print(a[0, :2])    # 同上, 但形狀不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1  # 返回一個 ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0337,  1.2891,  0.5059,  0.0979,  1.1066,  0.2501,  0.5826])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a > -0.5]  # 等值 a.masked_select(a > 1). 選擇結果不共享記憶體空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0337,  1.2891,  0.5059,  0.0979],\n",
       "        [ 1.1066, -1.1860,  0.2501, -1.4673]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行與第 1 行\n",
    "a[t.LongTensor([0, 1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他常用選擇函數整理如下表:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.index_select'>index_select(input, dim, index)</a> | 在指定維度 dim 上選取, 例如選取某些行, 某些列 |\n",
    "| <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_select'>masked_select(input, mask)</a> | 實例如上, 使用 ByteTensor 進行選取 |\n",
    "| non_zero(input) | 非 0 元素的索引 |\n",
    "| <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.gather'>gather(input, dim, index)</a> | 根據 index, 在 dim 維度 上選取資料, 輸出的 size 與 index 一樣 |\n",
    "\n",
    "gather 是一個比較複雜的操作, 對一個 二維 tensor, 輸出的每個元素如下:\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1\n",
    "```\n",
    "3D tensor 的 gather 操作同理, 下面舉幾個實例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選取對角線的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選取反對角線上的元素\n",
    "index = t.LongTensor([[3, 2, 1, 0]])\n",
    "index.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [12]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gather(1, index.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  9,  6,  3]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直覺也許你會使用下面方式取反對角線元素, 但結果卻...\n",
    "index = t.LongTensor([[3, 2, 1, 0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 2],\n",
       "        [2, 1],\n",
       "        [3, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選取兩個對角線上的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3],\n",
       "        [ 5,  6],\n",
       "        [10,  9],\n",
       "        [15, 12]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 進階索引\n",
    "PyTorch 0.2 版中增強了索引操作, 目前已經支援大多數 numpy 風格的進階索引. 進階索引可以看成是普通索引操作的擴充, 但進階索引操作的結果一般布和原始的 Tensor 共用記憶體."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0, 27).view(3, 3, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 24])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1, 2], [1, 2], [2, 0]]  # x[1, 1, 2], 和 x[2, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 10,  1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]]  # x[2, 0, 1], x[1, 0, 1], x[0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 1], ...]  # x[0] 和 x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 類型\n",
    "<b><a href='https://pytorch.org/docs/stable/tensors.html'>Tensor 有不同的資料類型</a></b>, 如下表所示, 每種類型分別對應有 CPU, GPU 版本 (<font color='brown'>HalfTensor 除外</font>). 預設的 tensor 是 <b>FloatTensor</b>, 可透過 <font color='blue'>t.set_default_tensor_type</font> 修改預設 tensor 類型 (<font color='brown'>如果預設類型為 GPU tensor, 則所有操作都將在 GPU 上進行</font>). Tensor 的類型對分析記憶體占用很有幫助. 舉例來說, 一個 size 為 (1000, 1000, 1000) 的 <b>FloatTensor</b>, 它有 1000 x 1000 x 1000 = 10^9 個元素, 每個元素佔 32/bit/8 = 4 bytes 記憶體, 所以共佔大約 4GB 記憶體/顯示卡記憶體. <b>HalfTensor</b> 是專門為 GPU 版本設計的, 同樣的元素個數, 顯示卡記憶體占用只有 FloatTensor 的一半, 所以可以相當大地緩解 GPU 顯示卡記憶體不足的問題, 但由於 <b>HalfTensor</b> 所能表示的數值大小與精度有限, 可能出現溢位等問題.\n",
    "<img src='https://github.com/johnklee/pytorch_tutorials/raw/master/ch03/images/T3-3.PNG'/>\n",
    "<center>表 3-3 tensor 資料類型</center>\n",
    "\n",
    "個資料類型之間可以互相轉換, <font color='blue'>type(<i>new_type</i>)</font> 是通用做法, 同時還有 float, long, half 等快速方法. CPU tensor 與 GPU tensor 之間的互相轉換透過 <b>tensor.cude</b> 和 <b>tensor.cpu</b> 的方法實現. Tensor 還有一個 <font color='blue'>new</font> 方法, 用法語 t.Tensor 一樣, 會呼叫該 tensor 對應類型的建置函數, 產生與目前 tensor 類型一致的的 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定預設 tensor, 注意參數是字符串\n",
    "# known issue:　https://github.com/pytorch/pytorch/issues/21989\n",
    "# t.set_default_tensor_type('torch.IntTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.4013e-45, 0.0000e+00, 1.4013e-45],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2, 3)\n",
    "print(a.type())   # 現在 a 是 FloatTensor\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.IntTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.int()\n",
    "print(b.type())\n",
    "b  # 把 a 轉換成 torch.IntTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.IntTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type_as(b)\n",
    "print(c.type())\n",
    "c  # c 的類型與 b 一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.4013e-45, 0.0000e+00, 1.4013e-45],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a.new(2, 3)  # 等值為 torch.FloatTensor(2, 3)\n",
    "print(d.type())\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.new??  # 查看函數 new 的原始碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逐元素操作\n",
    "這部分操作會對 tensor 的每一個元素 (<font color='brown'>point-wise, 又名 element-wise</font>) 進行操作, 這種操作的輸入與輸出形狀一致. 常用操作如下表:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| abs/sqrt/div/exp/fmod/log/pow... | 絕對值/平方根/除法/指數/求餘/求冪 |\n",
    "| cos/sin/asin/atan2/cosh | 三角函數 |\n",
    "| ceil/round/floor/trunc | 上取函數/四捨五入/下取函數/只保留整數部分 |\n",
    "| clamp(input, min, max) | 超過 min 和 max 部分截斷\n",
    "| sigmod/tanh... | 啟動函數 | \n",
    "\n",
    "對於很多操作, 例如 div, mul, pow, fmod 等, PyTorch 都實現了運算子多載, 所以可以直接使用運算子. 舉例來說, <font color='blue'>a ** 2</font> 相當於 <font color='blue'>torch.pow(a, 2)</font>; <font color='blue'>a * 2</font> 相等於 <font color='blue'>torch.mul(a, 2)</font>. <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.clamp'>clamp</a> 常用在某些需要比大小的地方, 如取一個 tensor 的每個元素與另一個數的較大值:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5403, -0.4161],\n",
       "        [-0.9900, -0.6536,  0.2837]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3).float()\n",
    "t.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [0., 1., 2.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3  # 等值於 t.fmod(a, 3): https://pytorch.org/docs/stable/torch.html#torch.fmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2  # 等值於 t.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a 中一個元素都與 3 相比, 取較大的一個\n",
    "print(a)\n",
    "t.clamp(a, min=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 歸併操作\n",
    "這種操作會使輸出形狀小於輸入形狀, 並可以沿著某一維度進行指定操作. 如加法 sum , 既可以計算整個 tensor 的合, 也可以計算 tensor 中每一行或每一列的合. 常用的歸併操作如下表所示:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| mean/sum/median/mode | 平均值/合/中位數/眾數 |\n",
    "| norm/dist | 範數/距離 |\n",
    "| std/var | 標準差/方差 |\n",
    "| cumsum/cumprod | 累加/累乘 |\n",
    "\n",
    "以上大多函數都有一個參數 <font color='violet'>dim</font> 用來指定這些操作是在哪個維度上執行的. 關於 <font color='violet'>dim</font> (<font color='brown'>對應於 Numpy 中的 axis</font>) 的解釋眾說紛紜, 這裡提供一個簡單的記憶方法. 假設輸入的形狀是 (m, n, k):\n",
    "* 如果指定 <font color='blue'>dim=0</font>, 輸出形狀就是 (1, n, k) 或 (n, k); \n",
    "* 如果指定 <font color='blue'>dim=1</font>, 輸出形狀就是 (m, 1, k) 或 (m, k); \n",
    "* 如果指定 <font color='blue'>dim=2</font>, 輸出形狀就是 (m, n, 1) 或 (m, n).\n",
    "\n",
    "size 中是否有 \"1\" 取決於參數 <font color='violet'>keepdim</font>, <font color='blue'>keepdim=True</font> 會保留維度 1. 從 Pytorch 0.2.0 版起, <font color='violet'>keepdim</font> 預設為 False. 注意! 以上只是經驗歸納, 並非所有函數都符合上面說明的形狀變化方式, 如 [cumsum](https://pytorch.org/docs/stable/torch.html#torch.cumsum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "print(b.type())\n",
    "print(b.shape)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b0_sum = b.sum(dim = 0, keepdim=True)\n",
    "print(b0_sum.shape)\n",
    "b0_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_sum = b.sum(dim = 1, keepdim=True)\n",
    "print(b1_sum.shape)\n",
    "b1_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_dft_sum = b.sum(dim = 1) # Default keepdim=False\n",
    "print(b1_dft_sum.shape)\n",
    "b1_dft_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[ 0,  1,  3],\n",
      "        [ 3,  7, 12],\n",
      "        [ 6, 13, 21]])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  5,  7],\n",
      "        [ 9, 12, 15]])\n"
     ]
    }
   ],
   "source": [
    "a = t.arange(0, 9).view(3, 3)\n",
    "print(a)\n",
    "print(a.cumsum(dim=1))  # 沿 row 累加\n",
    "print(a.cumsum(dim=0))  # 沿 col 累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比較\n",
    "比較函數中常有一些是逐元素比較, 操作類似逐元素操作, 還有一些規則類似歸併操作. 常用的比較函數如下表:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| gt/lt/ge/le/eq/ne | 大於/小於/大於等於/小於等於/等於/不等 |\n",
    "| topk | 最大的 k 個數 |\n",
    "| sort | 排序 |\n",
    "| max/min | 比較兩個 tensor 的最大和最小值 |\n",
    "\n",
    "表中第一行的比較操作已經實現了運算子的多載, 因此可以使用 a > b, a >= b, a < b 等操作, 其傳回結果為一個 <b>ByteTensor</b>, 可以用來選取元素. [max](https://pytorch.org/docs/stable/torch.html#torch.max)/[min](https://pytorch.org/docs/stable/torch.html#torch.min) 這兩個操作比較特殊, 以 [max](https://pytorch.org/docs/stable/torch.html#torch.max) 為例, 它有以下三種使用情況:\n",
    "* <b>t.max</b>(tensor): 傳回 tensor 中最大值\n",
    "* <b>t.max</b>(tensor, dim): 指定維度上最大的數, 傳回 tensor 和索引.\n",
    "* <b>t.max</b>(tensor1, tensor2): 比較兩個 tensor 相比較的較大元素."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  6.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "print(a.shape)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 6.,  3.,  0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "print(b.shape)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.ByteTensor\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = a > b\n",
    "print(r.type())\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 12., 15.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = a[ a > b]  # a 中 大於 b 的元素\n",
    "print(r.type())\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a)  # a 中最大元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15., 12.,  9.],\n",
      "        [ 6.,  3.,  0.]])\n",
      "tensor([15.,  6.])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "r1, r2 = t.max(b, dim=1) \n",
    "# 第一個 r1 返回值的 15 和 6 分別表示第 0 行 與第 1 行最大的元素\n",
    "# 第二個 r2 返回值的第一個 0 表示第一個最大值在第一行中的位置. 以此類推.\n",
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = t.max(a, b)  # 每個位置上取 max(a 元素, b 元素)\n",
    "print(r.type())\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10.],\n",
       "        [10., 12., 15.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = t.clamp(a, min=10)  # 每個位置上取 max(a 元素, 10)\n",
    "print(r.type())\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線性代數\n",
    "PyTorch 的線性函數主要封裝了 Blas 和 Lapack, 其用法和介面都與之類似. 常用的線性代數函數如下表:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| trace | 對角線元素之合 ([矩陣的跡](https://en.wikipedia.org/wiki/Matrix_(mathematics)#Trace)) |\n",
    "| diag | 對角線元素 |\n",
    "| triu/tril | 矩陣的 上三角, 下三角, 可指定偏移量 |\n",
    "| mm/bmm | 矩陣乘法, batch 的矩陣乘法 |\n",
    "| addmm/addbmm/addmv | 矩陣運算 |\n",
    "| t | [轉置](https://en.wikipedia.org/wiki/Transpose) |\n",
    "| dot/cross | 內積/外積 |\n",
    "| inverse | 求反矩陣 |\n",
    "| svd | 奇異值分解 | \n",
    "<center>表 3-7 常用的線性代數函數</center>\n",
    "\n",
    "這邊需要注意的是矩陣的轉置會導致儲存空間不連續, 需呼叫它的 [.contiguous](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous) 方法將其轉為連續."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  3.,  6.],\n",
      "        [ 9., 12., 15.]])\n",
      "tensor([[ 0.,  9.],\n",
      "        [ 3., 12.],\n",
      "        [ 6., 15.]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "b = a.t()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Tensor 和 Numpy</font>\n",
    "Tensor 和 Numpy 陣列之間有很高的相似性, 彼此之間的互動操作也非常簡單高效. 需要注意的是, <b>Numpy 和 Tensor 共用記憶體. 由於 Numpy 歷史悠久, 支援豐富的操作, 所以當遇到 Tensor 不支援的操作時, 可以先轉成 Numpy 陣列, 處理後再轉回 tensor, 其轉換負擔很小</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones([2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1. 100.   1.]\n",
      " [  1.   1.   1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "print(a)\n",
    "print(b)  # b 不受引響. b 與 a 不共用記憶體"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy()  # b 與 c 共用記憶體\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0, 1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 100.,   1.],\n",
      "        [  1.,   1.,   1.]])\n",
      "tensor([[  0., 100.,   1.],\n",
      "        [  1.,   1.,   1.]])\n",
      "[[  0. 100.   1.]\n",
      " [  1.   1.   1.]]\n"
     ]
    }
   ],
   "source": [
    "d = t.Tensor(c)\n",
    "print(d)\n",
    "d[0, 0] = 0\n",
    "print(d)\n",
    "print(c)  # c will be changed too. b, c, d 共用記憶體"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "廣播法則 (Broadcasting) 是科學運算中常用的技巧, 它在快速執行向量化同時不會占用額外記憶體, Numpy 的廣播法則定義如下:\n",
    "* (1) 讓所有輸入陣列都在其中 shape 最長的陣列看齊, shape 中不足的部分透過在前面加一補齊.\n",
    "* (2) 兩個陣列在某一個維度長度不一致時, 就是其中一個為一, 否則不能計算.\n",
    "* (3)當輸入陣列的某個維度的長度為 1 時, 計算時沿此維度複製擴充成一樣形狀.\n",
    "\n",
    "PyTorch 目前已經支援了自動廣播法則, 但是這裡還是建議透過以下兩個函數的組合實現廣播法則, 這樣更為直觀, 更不容易出錯.\n",
    "* [unsqueeze](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze) 或 view: 為資料某一維度的形狀補 1, 實現法則 (1)\n",
    "* [expand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand) 或 [expand_as](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand_as): 重複陣列, 實現法則 (3); 該操作不會複製陣列, 所以不會占用額外空間.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.ones(3, 2)\n",
    "b = t.zeros(2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自動廣播法則\n",
    "# 第一步: a 是二維, b 是三維, 所以先在較小的 a 前面補 1\n",
    "# 即 a.unsqueeze(0), a 的形狀變成 (1, 3, 2). b 的形狀是 (2, 3, 1)\n",
    "# 第二步: a 和 b 在第一, 三維的形狀不一樣, 其中一個為 1\n",
    "# 透過廣播法則擴充為 (2, 3, 2)\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手動廣播法則\n",
    "# 或者 a.view(1, 3, 2).expand(2, 3, 2) + b.expand(2, 3, 2)\n",
    "a.unsqueeze(0).expand(2, 3, 2) + b.expand(2, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>內部結構</font>\n",
    "tensor 的資料結構如下圖所示. tensor 分為標頭資訊區 (Tensor) 和儲存區 (Storage), 資訊區主要儲存 tensor 的形狀 (size), 步進值 (stride), 資料型態 (type) 等資訊, 而真正的資料則儲存成連續陣列. 由於資料動輒成千上萬, 因此資訊區元素占用記憶體較少, 主要記憶體占用取決於 tensor 中元素的數目, 即儲存區的大小.\n",
    "\n",
    "一般來說, 一個 tensor 具有與之相對應的 storgae, storage 是在 data 上面封裝的介面. 不同的 tensor 的標頭資訊一般不同, 但卻可能使用相同的 storage. 下面我們來看兩個實例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.LongStorage of size 6]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.LongStorage of size 6]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(2, 3)\n",
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一個對象的 id 值可以看做它在記憶體中的位址\n",
    "# storage 的記憶體位址一樣, 即是同一個 storage\n",
    "id(b.storage) == id(a.storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a 改變, b 也隨之改變, 因為他們共享記憶體\n",
    "a[1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 100\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.LongStorage of size 6]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[2:]\n",
    "c.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2567802837344, 2567802837328)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_ptr 返回 tensor 首元素的記憶體位址\n",
    "# 可以看出相差八, 這是因為 a 與 c 相差兩個元素, 一個元素佔 8 bytes, 故 2 * 4 = 8\n",
    "c.data_ptr(), a.data_ptr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  100, -100,    3,    4,    5])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = -100 # c[0] 記憶體位址對應 a[2] 的記憶體位址\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage_offset(), c.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = b[::2, ::2]  \n",
    "id(e.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b shape=torch.Size([2, 3])\n",
      "e shape=torch.Size([1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3, 1), (6, 2))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('b shape={}'.format(b.shape))\n",
    "print('e shape={}'.format(e.shape))\n",
    "b.stride(), e.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上可知絕大多數的操作並不修改 tensor 的資料, 只是修改了 tensor 的標頭資訊. 這種做法更節省記憶體, 同時提升了處理速度. 此外有些操作會導致 tensor 不連續, 這是需呼叫 [**tensor**.contiguous](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous) 方法將他們變成連續的資料, 該方法複製資料到新的記憶體, 不再與原來的資料共用 storage. 另外讀者可以思考一下, 之前說過的進階索引一般不可共用 storage, 而普通索引共用 storage, 這是為什麼呢? (提示: 普通索引可以透過修改 tensor 的 offset, stride 和 sizie 實現, 不修改 storage 的資料, 進階索引則不行)\n",
    "\n",
    "### <font color='green'>其他有關 Tensor 的話題 </font>\n",
    "\n",
    "\n",
    "### 持久化\n",
    "Tensor 的儲存與載入十分簡單, 使用 [t.save](https://pytorch.org/docs/stable/torch.html?highlight=t%20save#torch.save) 和 [t.load](https://pytorch.org/docs/stable/torch.html?highlight=t%20save#torch.load) 即可完成對應的功能. 在 save/load 時可指定使用的 pickle 模組, 在 load 時還可以將 GPU tensor 對映到 CPU 或其他 GPU 上:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = a.cuda(0)  # 把 a 轉為 GPU0 上的 tensor,\n",
    "    t.save(a, 'a.pth')\n",
    "    \n",
    "    # 加載為 b 並儲存在 GPU0 上\n",
    "    b = t.load('a.pth')\n",
    "    \n",
    "    # 加載為 c 並儲存在 CPU \n",
    "    c = t.load('a.pth', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量化\n",
    "向量化計算是一種特殊平行計算方式, 一般程式在同一時間只執行一個操作的方式, 它可在同一時間執行多個操作, 通常是對不同的資料執行同樣的一個或一批指令, 或說把指令應用於一個陣列/向量上. 向量化可相當大地加強科學運算效率, Python 本身是一種高階語言, 使用方便, 但許多操作很低效, 尤其是 for 循環. **在科學計算程式中應當極力避免使用 Python 原生的 for 循環, 而改用向量化的數值計算**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_loop_add(x, y):\n",
    "    result = []\n",
    "    for i, j in zip(x, y):\n",
    "        result.append(i + j)\n",
    "        \n",
    "    return t.Tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.92 ms ± 188 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "The slowest run took 7.88 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "11.7 µs ± 14.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = t.zeros(1000)\n",
    "y = t.ones(1000)\n",
    "\n",
    "%timeit -n 10 for_loop_add(x, y)\n",
    "%timeit -n 10 x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上可知兩者有超過 100 倍的速度差距, 因此在實際使用中應盡量呼叫內建函數, 這些函數底層由 C/C++ 實現, 能透過執行底層最佳化實現高效計算. 因此在平時寫程式時, 就應該養成向量化的思維習慣.\n",
    "\n",
    "此外還有以下幾點需要注意:\n",
    "* 大多數 t.function 都有一個參數 out, 這時產生的結果將儲存在 out 指定的 tensor 之中.\n",
    "* [t.set_num_threads](https://pytorch.org/docs/stable/torch.html#torch.set_num_threads) 可以設定 PyTorch 進行 CPU 多執行緒平行計算時所占用的執行緒, 用來限制 PyTorch 所占用的 CPU 數目.\n",
    "* [t.set_printoptions](https://pytorch.org/docs/stable/torch.html#torch.set_printoptions) 可以用來設定列印 tensor 時的數值精度和格式."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19999, dtype=torch.int32) tensor(19998, dtype=torch.int32)\n",
      "tensor(19999) tensor(19998)\n"
     ]
    }
   ],
   "source": [
    "a = t.IntTensor()\n",
    "t.arange(0, 20000, out=a)\n",
    "print(a[-1], a[-2])\n",
    "\n",
    "b = t.LongTensor()\n",
    "t.arange(0, 20000, out=b) # 64 bits 的 LongTensor 不會溢出\n",
    "print(b[-1], b[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9329, -1.2726, -1.3018],\n",
       "        [ 0.4908,  0.5335,  0.4666]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.93287075, -1.27256680, -1.30184746],\n",
       "        [ 0.49080709,  0.53354150,  0.46657836]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_printoptions(precision=8)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>小試牛刀</font>\n",
    "線性回歸是機器學習的入門知識, 應用十分廣泛. 其表達形式為 y = wx + b + e, 誤差 e 服從平均值為 0 的常態分佈:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定隨機數 seed, 保證在不同計算機上運行下的輸出一致.\n",
    "t.manual_seed(1000)\n",
    "\n",
    "def get_fake_data(batch_size=20):\n",
    "    ''' 產生隨機資料: y = x * 2 + 3 , 並加上一些雜訊'''\n",
    "    x = t.rand(batch_size, 1) * 20\n",
    "    y = x * 2 + (1 + t.randn(batch_size, 1)) * 3\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x255b1d93f60>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 來看看產生的 x-y 分布\n",
    "x, y = get_fake_data()\n",
    "plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.shape=torch.Size([1]); x.shape=torch.Size([20])\n",
      "0.5635384\n",
      "tensor([ 0.00000000,  0.56353837,  1.12707675,  1.69061518,  2.25415349,\n",
      "         2.81769180,  3.38123035,  3.94476867,  4.50830698,  5.07184553,\n",
      "         5.63538361,  6.19892216,  6.76246071,  7.32599878,  7.88953733,\n",
      "         8.45307541,  9.01661396,  9.58015251, 10.14369106, 10.70722866])\n"
     ]
    }
   ],
   "source": [
    "w = t.rand(1, 1)\n",
    "b = t.zeros(1, 1)\n",
    "x = t.arange(0, 20)\n",
    "print('b.shape={}; x.shape={}'.format(b.view(1).shape, x.shape))\n",
    "print(w.numpy()[0][0])\n",
    "print(x.float() * w.numpy()[0][0] + b.view(1).expand_as(x))\n",
    "#print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([20])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final w=tensor([[1.99517000]])\n",
      "Final b=tensor([[2.96910644]])\n"
     ]
    }
   ],
   "source": [
    "# 隨機初始化參數\n",
    "batch_size = 20\n",
    "w = t.rand(1, 1)\n",
    "b = t.zeros(1, 1)\n",
    "print('Initial w={}'.format(w))\n",
    "print('Initial b={}'.format(b))\n",
    "\n",
    "lr = 0.0001 # 學習率\n",
    "for ii in range(10000):\n",
    "    x, y = get_fake_data(batch_size=batch_size)\n",
    "    \n",
    "    # forward: 計算 loss\n",
    "    # print('x.shape={}; w.shape={}; y.shape={}'.format(x.shape, w.shape, y.shape))\n",
    "    y_pred = x.float().mm(w) + b.expand_as(y)\n",
    "    loss = 0.5 * (y_pred - y) ** 2  # 均方誤差\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    # backward: 手動計算梯度\n",
    "    dloss = 1\n",
    "    dy_pred = dloss * (y_pred - y)\n",
    "    \n",
    "    \n",
    "    dw = x.t().mm(dy_pred)\n",
    "    db = dy_pred.sum()\n",
    "    \n",
    "    # 更新參數\n",
    "    w.sub_(lr * dw)\n",
    "    b.sub_(lr * db)\n",
    "    \n",
    "    if ii % 2000 == 0:\n",
    "        print('Trained w={}'.format(w))\n",
    "        print('Trained b={}'.format(b))\n",
    "    \n",
    "        # 畫圖\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0, 20)\n",
    "        print('x.shape={}'.format(x.shape))\n",
    "        y = x.float() * w.numpy()[0][0] + b.view(1).expand_as(x)\n",
    "        plt.plot(x.squeeze().numpy(), y.squeeze().numpy(), color='red')  # prediction\n",
    "        \n",
    "        x2, y2 = get_fake_data(batch_size=batch_size)\n",
    "        plt.scatter(x2.numpy(), y2.numpy())  # true data\n",
    "        \n",
    "        plt.xlim(0, 20)\n",
    "        plt.ylim(0, 41)\n",
    "        plt.show()  # show graph\n",
    "        plt.pause(1)\n",
    "        \n",
    "print('Final w={}'.format(w))\n",
    "print('Final b={}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後的訓練結果, w 接近於 2, b 接近於 3, 並且圖中的直線和資料已經相當接近. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
