{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "幾乎所有的深度學習架構背後的設計核心都是張量和計算圖, PyTorch 也不例外, 本章我們將學習 PyTorch 中的張量系統 (Tensor) 和自動微分系統 (autograd).\n",
    "\n",
    "## <font color='darkblue'>Tensor</font>\n",
    "Tensor 又名張量, 它不僅在 PyTorch 中出現過, 也是 Theano, TensorFlow, Torch 和 MXNet 中重要的資料結構. 關於張量的本質不乏深度剖析的文章, 但從工程角度講, 可簡單地認為它就是一個陣列, 且支援高效的科學運算. 它可以是一個數 (純量), 一維陣列 (向量), 二維陣列 (矩陣) 或更高維的陣列 (高階資料). Tensor 和 <b><a href='http://www.numpy.org/'>numpy</a></b> 的 ndarrays 類似, 但 PyTorch 的 tensor 支援 GPU 加速.\n",
    "\n",
    "本節將系統說明 tensor 的使用, 力求面面俱到, 但不會涉及每個函數. 對於更多函數及其用法, 讀者可透過 IPython/Notebook 中使用 <function>? 檢視說明文件或參考 PyTorch 官方 API 文件.\n",
    "    \n",
    "### <font color='darkgreen'>基礎操作</font>\n",
    "學習過 numpy 的讀者會對本節內容非常熟悉, 因為 tensor 的介面設計與 <b><a href='http://www.numpy.org/'>numpy</a></b> 類似, 以方便使用者使用. 若不熟悉 <b><a href='http://www.numpy.org/'>numpy</a></b> 也沒關係, 本節內容會介紹. 從介面角度講, 對 tensor 的操作可以分為兩種:\n",
    "1. <a href='https://pytorch.org/docs/stable/torch.html'><b>torch</b></a> 上 function, 例如 <a href='https://pytorch.org/docs/stable/torch.html#torch.save'><b>torch</b>.save</a> 等.\n",
    "2. <a href='https://pytorch.org/docs/stable/tensors.html'><b>tensor</b></a> 上的 function, 如 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view'><b>tensor</b>.view</a> 等.\n",
    "\n",
    "為方便使用, 對 tensor 的大部分操作同時支援這兩個介面, 在這裡如 <font color='blue'>torch.sum(a, b)</font> 與 <font color='blue'>a.sum(b)</font> 功能相同. 從儲存的角度講, 對 tensor 的操作又可分成兩種:\n",
    "1. 不會修改本身的資料, 如 <font color='blue'>a.add(b)</font>, 加法的結果會回傳一個新的 tensor.\n",
    "2. 會修改本身資料 (inplace), 如 <font color='blue'>a.add_(b)</font>, 加法的結果會儲存到變數 a 中.\n",
    "\n",
    "函數名稱以 _ 結尾的都是 inplace 操作, 即會修改呼叫者自己本身的資料.\n",
    "\n",
    "### 建立 Tensor\n",
    "在 PyTorch 中新增 tensor 的方法有很多, 常見的方法整理如下:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| Tensor(*size) | 建構函式 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.ones'>ones(*size)</a> | 全部為 1 且長度為 <i>size</i> 的 Tensor |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.zeros'>zeros(*size)</a> | 全部為 0 且長度為 <i>size</i> 的 Tensor |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.eye'>eye(*size)</a> | 對角線為 1, 其他為 0 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.arange'>arange(s, e, step)</a> | 從 <i>s</i> 到 <i>e</i> 取 步進值 為 <i>step</i> |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.linspace'>linspace(s, e, steps)</a> | 從 <i>s</i> 到 <i>e</i> 均勻分成 <i>steps</i> 份 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.rand'>rand</a>/<a href='https://pytorch.org/docs/stable/torch.html#torch.randn'>randn(*size)</a> | 均勻/標準分布 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.normal'>normal(mean std)</a> / uniform(from, to) | 常態分布 / 均勻分布 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.randperm'>randperm(m)</a> | 隨機排列 |\n",
    "\n",
    "<br/>\n",
    "首先來介紹透過 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor'><b>Tensor</b></a> 建構子建立的 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor'><b>Tensor</b></a> 實例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "\n",
    "# 指定 tensor 的形狀為 2 rows, 3 columns. 內容值為記憶體當時的值\n",
    "a = t.Tensor(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 list 建立 tensor\n",
    "b = t.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把 tensor 轉回 list\n",
    "b.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size'>tensor.size()</a> 傳回 <font color='blue'><b>torch.Size</b></font> 物件, 它是 tuple 的字類別:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 傳回 b 中的元素數目: 2 * 3 = 6\n",
    "b.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.3649e+08,  4.5916e-41,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]), tensor([2., 3.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 創建一個與 b 形狀一樣的 tensor\n",
    "c = t.Tensor(b_size)\n",
    "\n",
    "# 創建一個元素為 2 和 3 的 tensor\n",
    "d = t.Tensor((2, 3))\n",
    "\n",
    "c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size'>tensor.size()</a>, 還可以利用 tensor.shape 直接檢視 tensor 的形狀:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是 t.Tensor(*size) 建立 tensor 時, 系統不會馬上分配空間, 只會計算剩餘的記憶體是否足夠使用, 使用到 tensor 時才會分配, 而其他操作都是在建立 tensor 後馬上進行空間分配. 其他常用的建立 tensor 方式舉例如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0162,  0.7413,  0.5159],\n",
       "        [ 0.2589, -2.0448,  1.1007]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 2, 3, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5) # 長度為 5 的隨機排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用的 Tensor 操作\n",
    "透過 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view'>tensor.view</a> 方法可以修改 tensor 的形狀, 但必須確定調整前後元素總數一致. view 不會修改本身的資料, 傳回的新 tensor 與原來的 tensor 共用記憶體, 即更改其中一個, 另一個也會跟著改變. 在實際應用場合可能經常需要增加或減少某一維度, 這時 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.squeeze'>squeeze</a> 和 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unsqueeze'>unsqueeze</a> 兩個函數就派上用場."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 當某一維度為 -1 時, 會自動計算它的大小\n",
    "b = a.view(-1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在第一維 (下標從 0 開始) 上增加 \"1\"\n",
    "b1 = b.unsqueeze(1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -2 表示倒數第二個維度\n",
    "b2 = b.unsqueeze(-2)\n",
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0, 1, 2],\n",
       "           [3, 4, 5]]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 2],\n",
       "          [3, 4, 5]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 壓縮第 0 維\n",
    "c1 = c.squeeze(0)\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把所有維度為 \"1\" 進行壓縮 \n",
    "c2 = c.squeeze()\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0, 100,   2,   3,   4,   5]), tensor([[  0, 100,   2],\n",
       "         [  3,   4,   5]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "\n",
    "# b 與 a 共用記憶體, 修改了 a, b 也會改變\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize 是另一種可用來調整 size 的方法, 但與 view 不同的是它可以 tensor 的尺寸. 如果新尺寸超過原尺寸, 會自動分配新的記憶體空間; 而如果尺寸小於原尺寸, 則之前的資料依舊會被儲存, 先來看一個範例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[            0,           100,             2],\n",
       "        [            3,             4,             5],\n",
       "        [            0,             0, 3180132821424]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 舊資料依舊保存著, 多出來的空間則為分配到的記憶體空間的值\n",
    "b.resize_(3, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引操作\n",
    "<a href='https://pytorch.org/docs/stable/tensors.html#'><b>Tensor</b></a> 支援與 <a href='https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html'><b>numpy.ndarray</b></a> 類似的索引操作, 語法上也類似, 下面透過範例說明常用的索引操作. 如無特殊說明, 索引出來的結果與 Tensor 共用記憶體, 即修改一個會反映到另一個:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1747,  0.2555, -2.8703,  0.7759],\n",
       "        [-0.8603, -0.1547, -0.2330, -0.3520],\n",
       "        [ 0.6643, -0.8511, -1.4744,  0.1645]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1747,  0.2555, -2.8703,  0.7759])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行 (下標從 0 開始)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1747, -0.8603,  0.6643])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 列\n",
    "a[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.8703)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行; 第 2 列 上的元素. 等同 a[0][2]\n",
    "a[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7759)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行, 最後一個元素\n",
    "a[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1747,  0.2555, -2.8703,  0.7759],\n",
       "        [-0.8603, -0.1547, -0.2330, -0.3520]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前兩行\n",
    "a[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1747,  0.2555],\n",
       "        [-0.8603, -0.1547]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前兩行, 第 0, 1 列\n",
    "a[:2, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1747,  0.2555]])\n",
      "tensor([-0.1747,  0.2555])\n"
     ]
    }
   ],
   "source": [
    "print(a[0:1, :2])  # 第 0 行, 前兩列\n",
    "print(a[0, :2])    # 同上, 但形狀不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1  # 返回一個 ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1747,  0.2555,  0.7759, -0.1547, -0.2330, -0.3520,  0.6643,  0.1645])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a > -0.5]  # 等值 a.masked_select(a > 1). 選擇結果不共享記憶體空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1747,  0.2555, -2.8703,  0.7759],\n",
       "        [-0.8603, -0.1547, -0.2330, -0.3520]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行與第 1 行\n",
    "a[t.LongTensor([0, 1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他常用選擇函數整理如下表:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.index_select'>index_select(input, dim, index)</a> | 在指定維度 dim 上選取, 例如選取某些行, 某些列 |\n",
    "| <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_select'>masked_select(input, mask)</a> | 實例如上, 使用 ByteTensor 進行選取 |\n",
    "| non_zero(input) | 非 0 元素的索引 |\n",
    "| <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.gather'>gather(input, dim, index)</a> | 根據 index, 在 dim 維度 上選取資料, 輸出的 size 與 index 一樣 |\n",
    "\n",
    "gather 是一個比較複雜的操作, 對一個 二維 tensor, 輸出的每個元素如下:\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1\n",
    "```\n",
    "3D tensor 的 gather 操作同理, 下面舉幾個實例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選取對角線的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選取反對角線上的元素\n",
    "index = t.LongTensor([[3, 2, 1, 0]])\n",
    "index.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [12]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gather(1, index.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  9,  6,  3]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直覺也許你會使用下面方式取反對角線元素, 但結果卻...\n",
    "index = t.LongTensor([[3, 2, 1, 0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 2],\n",
       "        [2, 1],\n",
       "        [3, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選取兩個對角線上的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3],\n",
       "        [ 5,  6],\n",
       "        [10,  9],\n",
       "        [15, 12]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 進階索引\n",
    "PyTorch 0.2 版中增強了索引操作, 目前已經支援大多數 numpy 風格的進階索引. 進階索引可以看成是普通索引操作的擴充, 但進階索引操作的結果一般布和原始的 Tensor 共用記憶體."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0, 27).view(3, 3, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 24])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1, 2], [1, 2], [2, 0]]  # x[1, 1, 2], 和 x[2, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 10,  1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]]  # x[2, 0, 1], x[1, 0, 1], x[0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 1], ...]  # x[0] 和 x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 類型\n",
    "<b><a href='https://pytorch.org/docs/stable/tensors.html'>Tensor 有不同的資料類型</a></b>, 如下表所示, 每種類型分別對應有 CPU, GPU 版本 (<font color='brown'>HalfTensor 除外</font>). 預設的 tensor 是 <b>FloatTensor</b>, 可透過 <font color='blue'>t.set_default_tensor_type</font> 修改預設 tensor 類型 (<font color='brown'>如果預設類型為 GPU tensor, 則所有操作都將在 GPU 上進行</font>). Tensor 的類型對分析記憶體占用很有幫助. 舉例來說, 一個 size 為 (1000, 1000, 1000) 的 <b>FloatTensor</b>, 它有 1000 x 1000 x 1000 = 10^9 個元素, 每個元素佔 32/bit/8 = 4 bytes 記憶體, 所以共佔大約 4GB 記憶體/顯示卡記憶體. <b>HalfTensor</b> 是專門為 GPU 版本設計的, 同樣的元素個數, 顯示卡記憶體占用只有 FloatTensor 的一半, 所以可以相當大地緩解 GPU 顯示卡記憶體不足的問題, 但由於 <b>HalfTensor</b> 所能表示的數值大小與精度有限, 可能出現溢位等問題.\n",
    "<img src='https://github.com/johnklee/pytorch_tutorials/raw/master/ch03/images/T3-3.PNG'/>\n",
    "<center>表 3-3 tensor 資料類型</center>\n",
    "\n",
    "個資料類型之間可以互相轉換, <font color='blue'>type(<i>new_type</i>)</font> 是通用做法, 同時還有 float, long, half 等快速方法. CPU tensor 與 GPU tensor 之間的互相轉換透過 <b>tensor.cude</b> 和 <b>tensor.cpu</b> 的方法實現. Tensor 還有一個 <font color='blue'>new</font> 方法, 用法語 t.Tensor 一樣, 會呼叫該 tensor 對應類型的建置函數, 產生與目前 tensor 類型一致的的 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only floating-point types are supported as the default type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-9f3ba9e990d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 設定預設 tensor, 注意參數是字符串\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# known issue:　https://github.com/pytorch/pytorch/issues/21989\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_default_tensor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'torch.IntTensor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\johnlee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36mset_default_tensor_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_string_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_import_dotted_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_default_tensor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only floating-point types are supported as the default type"
     ]
    }
   ],
   "source": [
    "# 設定預設 tensor, 注意參數是字符串\n",
    "# known issue:　https://github.com/pytorch/pytorch/issues/21989\n",
    "t.set_default_tensor_type('torch.IntTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3649e+08,  4.5916e-41,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2, 3)\n",
    "print(a.type())   # 現在 a 是 FloatTensor\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.IntTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-136485120,          0,          0],\n",
       "        [         0,          0,          0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.int()\n",
    "print(b.type())\n",
    "b  # 把 a 轉換成 torch.IntTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.IntTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-136485120,          0,          0],\n",
       "        [         0,          0,          0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type_as(b)\n",
    "print(c.type())\n",
    "c  # c 的類型與 b 一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3649e+08,  4.5916e-41,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a.new(2, 3)  # 等值為 torch.FloatTensor(2, 3)\n",
    "print(d.type())\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.new??  # 查看函數 new 的原始碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逐元素操作\n",
    "這部分操作會對 tensor 的每一個元素 (<font color='brown'>point-wise, 又名 element-wise</font>) 進行操作, 這種操作的輸入與輸出形狀一致. 常用操作如下表:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| abs/sqrt/div/exp/fmod/log/pow... | 絕對值/平方根/除法/指數/求餘/求冪 |\n",
    "| cos/sin/asin/atan2/cosh | 三角函數 |\n",
    "| ceil/round/floor/trunc | 上取函數/四捨五入/下取函數/只保留整數部分 |\n",
    "| clamp(input, min, max) | 超過 min 和 max 部分截斷\n",
    "| sigmod/tanh... | 啟動函數 | \n",
    "\n",
    "對於很多操作, 例如 div, mul, pow, fmod 等, PyTorch 都實現了運算子多載, 所以可以直接使用運算子. 舉例來說, <font color='blue'>a ** 2</font> 相當於 <font color='blue'>torch.pow(a, 2)</font>; <font color='blue'>a * 2</font> 相等於 <font color='blue'>torch.mul(a, 2)</font>. <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.clamp'>clamp</a> 常用在某些需要比大小的地方, 如取一個 tensor 的每個元素與另一個數的較大值:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5403, -0.4161],\n",
       "        [-0.9900, -0.6536,  0.2837]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3).float()\n",
    "t.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [0., 1., 2.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3  # 等值於 t.fmod(a, 3): https://pytorch.org/docs/stable/torch.html#torch.fmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2  # 等值於 t.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a 中一個元素都與 3 相比, 取較大的一個\n",
    "print(a)\n",
    "t.clamp(a, min=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 歸併操作\n",
    "這種操作會使輸出形狀小於輸入形狀, 並可以沿著某一維度進行指定操作. 如加法 sum , 既可以計算整個 tensor 的合, 也可以計算 tensor 中每一行或每一列的合. 常用的歸併操作如下表所示:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| mean/sum/median/mode | 平均值/合/中位數/眾數 |\n",
    "| norm/dist | 範數/距離 |\n",
    "| std/var | 標準差/方差 |\n",
    "| cumsum/cumprod | 累加/累乘 |\n",
    "\n",
    "以上大多函數都有一個參數 <font color='violet'>dim</font> 用來指定這些操作是在哪個維度上執行的. 關於 <font color='violet'>dim</font> (<font color='brown'>對應於 Numpy 中的 axis</font>) 的解釋眾說紛紜, 這裡提供一個簡單的記憶方法. 假設輸入的形狀是 (m, n, k):\n",
    "* 如果指定 <font color='blue'>dim=0</font>, 輸出形狀就是 (1, n, k) 或 (n, k); \n",
    "* 如果指定 <font color='blue'>dim=1</font>, 輸出形狀就是 (m, 1, k) 或 (m, k); \n",
    "* 如果指定 <font color='blue'>dim=2</font>, 輸出形狀就是 (m, n, 1) 或 (m, n).\n",
    "\n",
    "size 中是否有 \"1\" 取決於參數 <font color='violet'>keepdim</font>, <font color='blue'>keepdim=True</font> 會保留維度 1. 從 Pytorch 0.2.0 版起, <font color='violet'>keepdim</font> 預設為 False. 注意! 以上只是經驗歸納, 並非所有函數都符合上面說明的形狀變化方式, 如 [cumsum](https://pytorch.org/docs/stable/torch.html#torch.cumsum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "print(b.type())\n",
    "print(b.shape)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b0_sum = b.sum(dim = 0, keepdim=True)\n",
    "print(b0_sum.shape)\n",
    "b0_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_sum = b.sum(dim = 1, keepdim=True)\n",
    "print(b1_sum.shape)\n",
    "b1_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_dft_sum = b.sum(dim = 1) # Default keepdim=False\n",
    "print(b1_dft_sum.shape)\n",
    "b1_dft_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[ 0,  1,  3],\n",
      "        [ 3,  7, 12],\n",
      "        [ 6, 13, 21]])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  5,  7],\n",
      "        [ 9, 12, 15]])\n"
     ]
    }
   ],
   "source": [
    "a = t.arange(0, 9).view(3, 3)\n",
    "print(a)\n",
    "print(a.cumsum(dim=1))  # 沿 row 累加\n",
    "print(a.cumsum(dim=0))  # 沿 col 累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比較\n",
    "比較函數中常有一些是逐元素比較, 操作類似逐元素操作, 還有一些規則類似歸併操作. 常用的比較函數如下表:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| gt/lt/ge/le/eq/ne | 大於/小於/大於等於/小於等於/等於/不等 |\n",
    "| topk | 最大的 k 個數 |\n",
    "| sort | 排序 |\n",
    "| max/min | 比較兩個 tensor 的最大和最小值 |\n",
    "\n",
    "表中第一行的比較操作已經實現了運算子的多載, 因此可以使用 a > b, a >= b, a < b 等操作, 其傳回結果為一個 <b>ByteTensor</b>, 可以用來選取元素. [max](https://pytorch.org/docs/stable/torch.html#torch.max)/[min](https://pytorch.org/docs/stable/torch.html#torch.min) 這兩個操作比較特殊, 以 [max](https://pytorch.org/docs/stable/torch.html#torch.max) 為例, 它有以下三種使用情況:\n",
    "* <b>t.max</b>(tensor): 傳回 tensor 中最大值\n",
    "* <b>t.max</b>(tensor, dim): 指定維度上最大的數, 傳回 tensor 和索引.\n",
    "* <b>t.max</b>(tensor1, tensor2): 比較兩個 tensor 相比較的較大元素."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  6.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "print(a.shape)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 6.,  3.,  0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "print(b.shape)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.ByteTensor\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = a > b\n",
    "print(r.type())\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 12., 15.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = a[ a > b]  # a 中 大於 b 的元素\n",
    "print(r.type())\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a)  # a 中最大元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15., 12.,  9.],\n",
      "        [ 6.,  3.,  0.]])\n",
      "tensor([15.,  6.])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "r1, r2 = t.max(b, dim=1) \n",
    "# 第一個 r1 返回值的 15 和 6 分別表示第 0 行 與第 1 行最大的元素\n",
    "# 第二個 r2 返回值的第一個 0 表示第一個最大值在第一行中的位置. 以此類推.\n",
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = t.max(a, b)  # 每個位置上取 max(a 元素, b 元素)\n",
    "print(r.type())\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10.],\n",
       "        [10., 12., 15.]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = t.clamp(a, min=10)  # 每個位置上取 max(a 元素, 10)\n",
    "print(r.type())\n",
    "print(r.shape)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線性代數\n",
    "PyTorch 的線性函數主要封裝了 Blas 和 Lapack, 其用法和介面都與之類似. 常用的線性代數函數如下表:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| trace | 對角線元素之合 ([矩陣的跡](https://en.wikipedia.org/wiki/Matrix_(mathematics)#Trace)) |\n",
    "| diag | 對角線元素 |\n",
    "| triu/tril | 矩陣的 上三角, 下三角, 可指定偏移量 |\n",
    "| mm/bmm | 矩陣乘法, batch 的矩陣乘法 |\n",
    "| addmm/addbmm/addmv | 矩陣運算 |\n",
    "| t | [轉置](https://en.wikipedia.org/wiki/Transpose) |\n",
    "| dot/cross | 內積/外積 |\n",
    "| inverse | 求反矩陣 |\n",
    "| svd | 奇異值分解 | \n",
    "<center>表 3-7 常用的線性代數函數</center>\n",
    "\n",
    "這邊需要注意的是矩陣的轉置會導致儲存空間不連續, 需呼叫它的 [.contiguous](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous) 方法將其轉為連續."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  3.,  6.],\n",
      "        [ 9., 12., 15.]])\n",
      "tensor([[ 0.,  9.],\n",
      "        [ 3., 12.],\n",
      "        [ 6., 15.]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "b = a.t()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  9.],\n",
       "        [ 3., 12.],\n",
       "        [ 6., 15.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Tensor 和 Numpy</font>\n",
    "Tensor 和 Numpy 陣列之間有很高的相似性, 彼此之間的互動操作也非常簡單高效. 需要注意的是, <b>Numpy 和 Tensor 共用記憶體. 由於 Numpy 歷史悠久, 支援豐富的操作, 所以當遇到 Tensor 不支援的操作時, 可以先轉成 Numpy 陣列, 處理後再轉回 tensor, 其轉換負擔很小</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones([2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1. 100.   1.]\n",
      " [  1.   1.   1.]]\n",
      "tensor([[  0., 100.,   1.],\n",
      "        [  1.,   1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "print(a)\n",
    "print(b)  # b 不受引響. b 與 a 不共用記憶體"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy()  # b 與 c 共用記憶體\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0, 1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0., 100.,   1.],\n",
      "        [  1.,   1.,   1.]])\n",
      "tensor([[  0., 100.,   1.],\n",
      "        [  1.,   1.,   1.]])\n",
      "[[  0. 100.   1.]\n",
      " [  1.   1.   1.]]\n"
     ]
    }
   ],
   "source": [
    "d = t.Tensor(c)\n",
    "print(d)\n",
    "d[0, 0] = 0\n",
    "print(d)\n",
    "print(c)  # c will be changed too. b, c, d 共用記憶體"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
