{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "幾乎所有的深度學習架構背後的設計核心都是張量和計算圖, PyTorch 也不例外, 本章我們將學習 PyTorch 中的張量系統 (Tensor) 和自動微分系統 (autograd).\n",
    "\n",
    "## <font color='darkblue'>Tensor</font>\n",
    "Tensor 又名張量, 它不僅在 PyTorch 中出現過, 也是 Theano, TensorFlow, Torch 和 MXNet 中重要的資料結構. 關於張量的本質不乏深度剖析的文章, 但從工程角度講, 可簡單地認為它就是一個陣列, 且支援高效的科學運算. 它可以是一個數 (純量), 一維陣列 (向量), 二維陣列 (矩陣) 或更高維的陣列 (高階資料). Tensor 和 <b><a href='http://www.numpy.org/'>numpy</a></b> 的 ndarrays 類似, 但 PyTorch 的 tensor 支援 GPU 加速.\n",
    "\n",
    "本節將系統說明 tensor 的使用, 力求面面俱到, 但不會涉及每個函數. 對於更多函數及其用法, 讀者可透過 IPython/Notebook 中使用 <function>? 檢視說明文件或參考 PyTorch 官方 API 文件.\n",
    "    \n",
    "### <font color='darkgreen'>基礎操作</font>\n",
    "學習過 numpy 的讀者會對本節內容非常熟悉, 因為 tensor 的介面設計與 <b><a href='http://www.numpy.org/'>numpy</a></b> 類似, 以方便使用者使用. 若不熟悉 <b><a href='http://www.numpy.org/'>numpy</a></b> 也沒關係, 本節內容會介紹. 從介面角度講, 對 tensor 的操作可以分為兩種:\n",
    "1. <a href='https://pytorch.org/docs/stable/torch.html'><b>torch</b></a> 上 function, 例如 <a href='https://pytorch.org/docs/stable/torch.html#torch.save'><b>torch</b>.save</a> 等.\n",
    "2. <a href='https://pytorch.org/docs/stable/tensors.html'><b>tensor</b></a> 上的 function, 如 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view'><b>tensor</b>.view</a> 等.\n",
    "\n",
    "為方便使用, 對 tensor 的大部分操作同時支援這兩個介面, 在這裡如 <font color='blue'>torch.sum(a, b)</font> 與 <font color='blue'>a.sum(b)</font> 功能相同. 從儲存的角度講, 對 tensor 的操作又可分成兩種:\n",
    "1. 不會修改本身的資料, 如 <font color='blue'>a.add(b)</font>, 加法的結果會回傳一個新的 tensor.\n",
    "2. 會修改本身資料 (inplace), 如 <font color='blue'>a.add_(b)</font>, 加法的結果會儲存到變數 a 中.\n",
    "\n",
    "函數名稱以 _ 結尾的都是 inplace 操作, 即會修改呼叫者自己本身的資料.\n",
    "\n",
    "### 建立 Tensor\n",
    "在 PyTorch 中新增 tensor 的方法有很多, 常見的方法整理如下:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| Tensor(*size) | 建構函式 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.ones'>ones(*size)</a> | 全部為 1 且長度為 <i>size</i> 的 Tensor |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.zeros'>zeros(*size)</a> | 全部為 0 且長度為 <i>size</i> 的 Tensor |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.eye'>eye(*size)</a> | 對角線為 1, 其他為 0 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.arange'>arange(s, e, step)</a> | 從 <i>s</i> 到 <i>e</i> 取 步進值 為 <i>step</i> |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.linspace'>linspace(s, e, steps)</a> | 從 <i>s</i> 到 <i>e</i> 均勻分成 <i>steps</i> 份 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.rand'>rand</a>/<a href='https://pytorch.org/docs/stable/torch.html#torch.randn'>randn(*size)</a> | 均勻/標準分布 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.normal'>normal(mean std)</a> / uniform(from, to) | 常態分布 / 均勻分布 |\n",
    "| <a href='https://pytorch.org/docs/stable/torch.html#torch.randperm'>randperm(m)</a> | 隨機排列 |\n",
    "\n",
    "<br/>\n",
    "首先來介紹透過 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor'><b>Tensor</b></a> 建構子建立的 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor'><b>Tensor</b></a> 實例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "\n",
    "# 指定 tensor 的形狀為 2 rows, 3 columns. 內容值為記憶體當時的值\n",
    "a = t.Tensor(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 list 建立 tensor\n",
    "b = t.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把 tensor 轉回 list\n",
    "b.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size'>tensor.size()</a> 傳回 <font color='blue'><b>torch.Size</b></font> 物件, 它是 tuple 的字類別:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 傳回 b 中的元素數目: 2 * 3 = 6\n",
    "b.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.3649e+08,  4.5916e-41,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]), tensor([2., 3.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 創建一個與 b 形狀一樣的 tensor\n",
    "c = t.Tensor(b_size)\n",
    "\n",
    "# 創建一個元素為 2 和 3 的 tensor\n",
    "d = t.Tensor((2, 3))\n",
    "\n",
    "c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size'>tensor.size()</a>, 還可以利用 tensor.shape 直接檢視 tensor 的形狀:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是 t.Tensor(*size) 建立 tensor 時, 系統不會馬上分配空間, 只會計算剩餘的記憶體是否足夠使用, 使用到 tensor 時才會分配, 而其他操作都是在建立 tensor 後馬上進行空間分配. 其他常用的建立 tensor 方式舉例如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0162,  0.7413,  0.5159],\n",
       "        [ 0.2589, -2.0448,  1.1007]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 2, 3, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5) # 長度為 5 的隨機排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用的 Tensor 操作\n",
    "透過 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view'>tensor.view</a> 方法可以修改 tensor 的形狀, 但必須確定調整前後元素總數一致. view 不會修改本身的資料, 傳回的新 tensor 與原來的 tensor 共用記憶體, 即更改其中一個, 另一個也會跟著改變. 在實際應用場合可能經常需要增加或減少某一維度, 這時 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.squeeze'>squeeze</a> 和 <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unsqueeze'>unsqueeze</a> 兩個函數就派上用場."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 當某一維度為 -1 時, 會自動計算它的大小\n",
    "b = a.view(-1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在第一維 (下標從 0 開始) 上增加 \"1\"\n",
    "b1 = b.unsqueeze(1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -2 表示倒數第二個維度\n",
    "b2 = b.unsqueeze(-2)\n",
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0, 1, 2],\n",
       "           [3, 4, 5]]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 2],\n",
       "          [3, 4, 5]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 壓縮第 0 維\n",
    "c1 = c.squeeze(0)\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把所有維度為 \"1\" 進行壓縮 \n",
    "c2 = c.squeeze()\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0, 100,   2,   3,   4,   5]), tensor([[  0, 100,   2],\n",
       "         [  3,   4,   5]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "\n",
    "# b 與 a 共用記憶體, 修改了 a, b 也會改變\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize 是另一種可用來調整 size 的方法, 但與 view 不同的是它可以 tensor 的尺寸. 如果新尺寸超過原尺寸, 會自動分配新的記憶體空間; 而如果尺寸小於原尺寸, 則之前的資料依舊會被儲存, 先來看一個範例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[            0,           100,             2],\n",
       "        [            3,             4,             5],\n",
       "        [            0,             0, 3180132821424]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 舊資料依舊保存著, 多出來的空間則為分配到的記憶體空間的值\n",
    "b.resize_(3, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引操作\n",
    "<a href='https://pytorch.org/docs/stable/tensors.html#'><b>Tensor</b></a> 支援與 <a href='https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html'><b>numpy.ndarray</b></a> 類似的索引操作, 語法上也類似, 下面透過範例說明常用的索引操作. 如無特殊說明, 索引出來的結果與 Tensor 共用記憶體, 即修改一個會反映到另一個:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1747,  0.2555, -2.8703,  0.7759],\n",
       "        [-0.8603, -0.1547, -0.2330, -0.3520],\n",
       "        [ 0.6643, -0.8511, -1.4744,  0.1645]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1747,  0.2555, -2.8703,  0.7759])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行 (下標從 0 開始)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1747, -0.8603,  0.6643])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 列\n",
    "a[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.8703)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行; 第 2 列 上的元素. 等同 a[0][2]\n",
    "a[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7759)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行, 最後一個元素\n",
    "a[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1747,  0.2555, -2.8703,  0.7759],\n",
       "        [-0.8603, -0.1547, -0.2330, -0.3520]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前兩行\n",
    "a[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1747,  0.2555],\n",
       "        [-0.8603, -0.1547]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前兩行, 第 0, 1 列\n",
    "a[:2, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1747,  0.2555]])\n",
      "tensor([-0.1747,  0.2555])\n"
     ]
    }
   ],
   "source": [
    "print(a[0:1, :2])  # 第 0 行, 前兩列\n",
    "print(a[0, :2])    # 同上, 但形狀不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1  # 返回一個 ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1747,  0.2555,  0.7759, -0.1547, -0.2330, -0.3520,  0.6643,  0.1645])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a > -0.5]  # 等值 a.masked_select(a > 1). 選擇結果不共享記憶體空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1747,  0.2555, -2.8703,  0.7759],\n",
       "        [-0.8603, -0.1547, -0.2330, -0.3520]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 行與第 1 行\n",
    "a[t.LongTensor([0, 1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他常用選擇函數整理如下表:\n",
    "\n",
    "| 函數 | 功能 |\n",
    "| --- | --- |\n",
    "| <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.index_select'>index_select(input, dim, index)</a> | 在指定維度 dim 上選取, 例如選取某些行, 某些列 |\n",
    "| <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_select'>masked_select(input, mask)</a> | 實例如上, 使用 ByteTensor 進行選取 |\n",
    "| non_zero(input) | 非 0 元素的索引 |\n",
    "| <a href='https://pytorch.org/docs/stable/tensors.html#torch.Tensor.gather'>gather(input, dim, index)</a> | 根據 index, 在 dim 維度 上選取資料, 輸出的 size 與 index 一樣 |\n",
    "\n",
    "gather 是一個比較複雜的操作, 對一個 二維 tensor, 輸出的每個元素如下:\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1\n",
    "```\n",
    "3D tensor 的 gather 操作同理, 下面舉幾個實例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選取對角線的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選取反對角線上的元素\n",
    "index = t.LongTensor([[3, 2, 1, 0]])\n",
    "index.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [12]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gather(1, index.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  9,  6,  3]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直覺也許你會使用下面方式取反對角線元素, 但結果卻...\n",
    "index = t.LongTensor([[3, 2, 1, 0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 2],\n",
       "        [2, 1],\n",
       "        [3, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選取兩個對角線上的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3],\n",
       "        [ 5,  6],\n",
       "        [10,  9],\n",
       "        [15, 12]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 進階索引\n",
    "PyTorch 0.2 版中增強了索引操作, 目前已經支援大多數 numpy 風格的進階索引. 進階索引可以看成是普通索引操作的擴充, 但進階索引操作的結果一般布和原始的 Tensor 共用記憶體."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0, 27).view(3, 3, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 24])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1, 2], [1, 2], [2, 0]]  # x[1, 1, 2], 和 x[2, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 10,  1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]]  # x[2, 0, 1], x[1, 0, 1], x[0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 1], ...]  # x[0] 和 x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 類型\n",
    "<b><a href='https://pytorch.org/docs/stable/tensors.html'>Tensor 有不同的資料類型</a></b>, 如下表所示, 每種類型分別對應有 CPU, GPU 版本 (HalfTensor 除外). 預設的 tensor 是 FloatTensor, 可透過"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
